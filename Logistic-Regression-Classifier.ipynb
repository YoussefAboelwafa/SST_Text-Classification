{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from utils import *\n",
    "\n",
    "EXP_NAME = \"Logistic Regression\"\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset from CSV file\n",
    "\n",
    "CSV file is modified to have columns as follows:\n",
    "\n",
    "- sentence: original sentence\n",
    "- label : class (0 to 4)\n",
    "- tokens : tokenized sentence\n",
    "- score : sentiment score (0 to 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"dataset/train.csv\")\n",
    "print(\"Train size: \", len(train_df))\n",
    "\n",
    "val_df = pd.read_csv(\"dataset/val.csv\")\n",
    "print(\"Val size  : \", len(val_df))\n",
    "\n",
    "test_df = pd.read_csv(\"dataset/test.csv\")\n",
    "print(\"Test size : \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train class distribution:\")\n",
    "print(train_df.label.value_counts().sort_index())\n",
    "\n",
    "print(\"Val class distribution:\")\n",
    "print(val_df.label.value_counts().sort_index())\n",
    "\n",
    "print(\"Test class distribution:\")\n",
    "print(test_df.label.value_counts().sort_index())\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "train_df[\"label\"].value_counts().sort_index().plot(kind=\"bar\", color=\"skyblue\")\n",
    "plt.title(\"Train Dataset Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "val_df[\"label\"].value_counts().sort_index().plot(kind=\"bar\", color=\"lightgreen\")\n",
    "plt.title(\"Val Dataset Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "test_df[\"label\"].value_counts().sort_index().plot(kind=\"bar\", color=\"salmon\")\n",
    "plt.title(\"Test Dataset Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokens):\n",
    "    return tokens.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence:\")\n",
    "print(train_df.iloc[3][\"sentence\"])\n",
    "\n",
    "print(\"\\nSentence Tokenized:\")\n",
    "tokens = tokenize(train_df.iloc[3][\"sentence\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(sentence):\n",
    "    words = tokenize(sentence)\n",
    "    return [(words[i], words[i + 1]) for i in range(len(words) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matrix for Train dataset\n",
    "\n",
    "using int8 instead of float64 to save memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_features(dataframe):\n",
    "    unique_bigrams = set()\n",
    "    for _, sample in dataframe.iterrows():\n",
    "        sample_bigrams = extract_bigrams(sample[\"sentence\"])\n",
    "        unique_bigrams.update(sample_bigrams)\n",
    "\n",
    "    bigrams_indices = {bi_gram: index for index, bi_gram in enumerate(unique_bigrams)}\n",
    "\n",
    "    feature_matrix = np.zeros((len(dataframe), len(unique_bigrams))).astype(np.int8)\n",
    "\n",
    "    for i, (_, sample) in enumerate(dataframe.iterrows()):\n",
    "        sample_bigrams = extract_bigrams(sample[\"sentence\"])\n",
    "        for b in sample_bigrams:\n",
    "            index = bigrams_indices.get(b)\n",
    "            if index is not None:\n",
    "                feature_matrix[i, index] = 1\n",
    "\n",
    "    return feature_matrix, bigrams_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_matrix, train_bigrams_indices = generate_train_features(train_df)\n",
    "print(\n",
    "    \"Train Feature matrix shape (samples x unique bigrams):\", train_feature_matrix.shape\n",
    ")\n",
    "print(\"Train Feature matrix datatype:\", train_feature_matrix.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify features for val & test set\n",
    "\n",
    "Remove all Features (bigrams) from val & test set that doesn't appear in training features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(dataframe):\n",
    "    unique_dataframe_bigrams = set()\n",
    "    for _, sample in dataframe.iterrows():\n",
    "        sample_bigrams = extract_bigrams(sample[\"sentence\"])\n",
    "        unique_dataframe_bigrams.update(sample_bigrams)\n",
    "\n",
    "    unique_train_bigrams = set(train_bigrams_indices.keys())\n",
    "    common_bigrams = unique_dataframe_bigrams.intersection(unique_train_bigrams)\n",
    "    feature_matrix = np.zeros((len(dataframe), len(train_bigrams_indices))).astype(\n",
    "        np.int8\n",
    "    )\n",
    "\n",
    "    for i, (_, sample) in enumerate(dataframe.iterrows()):\n",
    "        sample_bigrams = extract_bigrams(sample[\"sentence\"])\n",
    "        for b in sample_bigrams:\n",
    "            if b in common_bigrams:\n",
    "                index = train_bigrams_indices.get(b)\n",
    "                feature_matrix[i, index] = 1\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_feature_matrix = filter_features(val_df)\n",
    "print(\"Val Feature matrix shape (samples x unique bigrams):\", val_feature_matrix.shape)\n",
    "\n",
    "test_feature_matrix = filter_features(test_df)\n",
    "print(\n",
    "    \"\\nTest Feature matrix shape (samples x unique bigrams):\",\n",
    "    test_feature_matrix.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_feature_matrix\n",
    "y_train = train_df.label.values\n",
    "\n",
    "X_val = val_feature_matrix\n",
    "y_val = val_df.label.values\n",
    "\n",
    "X_test = test_feature_matrix\n",
    "y_test = test_df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comet Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"rwyMmTQC0QDIH0oF5XaSzgmh4\",\n",
    "    project_name=\"nlp-lr\",\n",
    "    workspace=\"youssefaboelwafa\",\n",
    ")\n",
    "experiment.set_name(EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class LR:\n",
    "    def __init__(self, num_classes=5, epochs=100, learning_rate=0.01):\n",
    "        self.num_classes = num_classes\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        n_classes = self.num_classes\n",
    "\n",
    "        self.weights = np.zeros((n_features, n_classes)).astype(np.float32)\n",
    "        self.bias = np.zeros(n_classes).astype(np.float32)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "\n",
    "            logits = X_train @ self.weights + self.bias\n",
    "            probabilities = softmax(logits)\n",
    "\n",
    "            output_hot_encoded = np.eye(n_classes)[y_train]\n",
    "\n",
    "            error = probabilities - output_hot_encoded\n",
    "\n",
    "            train_loss = (\n",
    "                -np.sum(output_hot_encoded * np.log(probabilities + 1e-15)) / n_samples\n",
    "            )\n",
    "\n",
    "            train_preds = np.argmax(probabilities, axis=1)\n",
    "            train_accuracy = np.mean(train_preds == y_train)\n",
    "\n",
    "            experiment.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            experiment.log_metric(\"train_accuracy\", train_accuracy, step=epoch)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X_train.T, error)\n",
    "            db = (1 / n_samples) * np.sum(error, axis=0)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_logits = X_val @ self.weights + self.bias\n",
    "                val_probabilities = softmax(val_logits)\n",
    "                val_output_hot_encoded = np.eye(n_classes)[y_val]\n",
    "\n",
    "                val_loss = (\n",
    "                    -np.sum(val_output_hot_encoded * np.log(val_probabilities + 1e-15))\n",
    "                    / X_val.shape[0]\n",
    "                )\n",
    "                val_preds = np.argmax(val_probabilities, axis=1)\n",
    "                val_accuracy = np.mean(val_preds == y_val)\n",
    "\n",
    "                experiment.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "                experiment.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "        print(\"Training completed\")\n",
    "        print(\"Train loss: \", train_loss)\n",
    "        print(\"Train accuracy: \", train_accuracy)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            print(\"Val loss: \", val_loss)\n",
    "            print(\"Val accuracy: \", val_accuracy)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.dot(X, self.weights) + self.bias\n",
    "        probabilities = softmax(scores)\n",
    "        preds = np.argmax(probabilities, axis=1)\n",
    "        return preds\n",
    "\n",
    "    def save_weights(self, weights_path, bias_path):\n",
    "        np.save(weights_path, self.weights)\n",
    "        np.save(bias_path, self.bias)\n",
    "\n",
    "    def load_weights(self, weights_path, bias_path):\n",
    "        self.weights = np.load(weights_path)\n",
    "        self.bias = np.load(bias_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR()\n",
    "# model.fit(X_train, y_train, X_val, y_val)\n",
    "model.load_weights(\"weights.npy\", \"bias.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classification_report(y_test, y_pred, target_names=[\"0\", \"1\", \"2\", \"3\", \"4\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=EPOCHS)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy : \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classification_report(y_test, y_pred, target_names=[\"0\", \"1\", \"2\", \"3\", \"4\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_sklearn = SGDClassifier(\n",
    "    loss=\"log_loss\", max_iter=EPOCHS, learning_rate=\"constant\", eta0=0.05, shuffle=True\n",
    ")\n",
    "\n",
    "sgd_sklearn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_sklearn.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy : \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classification_report(y_test, y_pred, target_names=[\"0\", \"1\", \"2\", \"3\", \"4\"])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
