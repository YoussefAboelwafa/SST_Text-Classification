{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T23:50:27.747299Z",
     "start_time": "2024-10-30T23:50:27.678910Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:09:51.537958Z",
     "start_time": "2024-10-31T00:09:50.727449Z"
    }
   },
   "source": [
    "def generate_vocab(train_df):\n",
    "    tokens = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        row_tokens = row['sentence'].split()\n",
    "        tokens.extend(row_tokens)\n",
    "    return np.unique(tokens) \n",
    "\n",
    "def get_number_of_words_in_class(df):\n",
    "    class_count = {}\n",
    "    for _, row in df.iterrows():\n",
    "        row_tokens = row['sentence'].split()\n",
    "        if row['label'] in class_count:\n",
    "            class_count[row['label']] += len(row_tokens)\n",
    "        else:\n",
    "            class_count[row['label']] = len(row_tokens)\n",
    "    return class_count   \n",
    "\n",
    "# generate vocabulary\n",
    "vocab = generate_vocab(train_df)\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "train_df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  The Rock is destined to be the 21st Century 's...      3   \n",
       "1  The gorgeously elaborate continuation of `` Th...      4   \n",
       "2  Singer\\/composer Bryan Adams contributes a sle...      3   \n",
       "3  You 'd think by now America would have had eno...      2   \n",
       "4               Yet the act is still charming here .      3   \n",
       "\n",
       "                                              tokens    score  \n",
       "0  The|Rock|is|destined|to|be|the|21st|Century|'s...  0.69444  \n",
       "1  The|gorgeously|elaborate|continuation|of|``|Th...  0.83333  \n",
       "2  Singer\\/composer|Bryan|Adams|contributes|a|sle...  0.62500  \n",
       "3  You|'d|think|by|now|America|would|have|had|eno...  0.50000  \n",
       "4               Yet|the|act|is|still|charming|here|.  0.72222  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>3</td>\n",
       "      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>4</td>\n",
       "      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>3</td>\n",
       "      <td>Singer\\/composer|Bryan|Adams|contributes|a|sle...</td>\n",
       "      <td>0.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>2</td>\n",
       "      <td>You|'d|think|by|now|America|would|have|had|eno...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>3</td>\n",
       "      <td>Yet|the|act|is|still|charming|here|.</td>\n",
       "      <td>0.72222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:11:48.037631Z",
     "start_time": "2024-10-31T00:10:09.586203Z"
    }
   },
   "source": [
    "n_doc = len(train_df)\n",
    "log_prior = np.log(train_df['label'].value_counts().sort_index() / n_doc).tolist()\n",
    "log_likelihood = np.zeros((len(vocab), 5))\n",
    "\n",
    "class_count = get_number_of_words_in_class(train_df)\n",
    "\n",
    "# Precompute counts for each word in each class\n",
    "for c in range(5):  # 5 classes\n",
    "    # Filter once by class\n",
    "    class_df = train_df[train_df['label'] == c]\n",
    "    \n",
    "    # Count occurrences of each word in vocab for this class\n",
    "    word_counts = {w: class_df['sentence'].apply(lambda x: x.count(w)).sum() for w in vocab}\n",
    "    \n",
    "    # Calculate log likelihood for each word in vocab\n",
    "    for index, w in enumerate(vocab):\n",
    "        n_wc = word_counts[w]\n",
    "        log_likelihood[index][c] = np.log((n_wc + 1) / (class_count[c] + len(vocab)))\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T00:11:52.987763Z",
     "start_time": "2024-10-31T00:11:52.980543Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
    "    # Create a dictionary to map words to their indices for quick lookup\n",
    "    word_to_index = {word: i for i, word in enumerate(V)}\n",
    "    sum_scores = np.zeros(len(C))  # Initialize scores for each class\n",
    "    \n",
    "    for c in range(len(C)):\n",
    "        # Initialize score with the log prior for the class\n",
    "        sum_scores[c] = logprior[c]\n",
    "        \n",
    "        for word in testdoc.split():  # Split the test document into words\n",
    "            if word in word_to_index:  # Check if the word is in V\n",
    "                # Update score by adding loglikelihood of the word given the class\n",
    "                index = np.where(vocab == word)[0]\n",
    "                sum_scores[c] += loglikelihood[index[0]][c]\n",
    "                \n",
    "    # Return the class with the maximum score\n",
    "    return np.argmax(sum_scores)\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T23:53:51.142947Z",
     "start_time": "2024-10-30T23:52:00.945381Z"
    }
   },
   "source": [
    "cnt=0\n",
    "for _, row in train_df.iterrows():\n",
    "    predicted_class=test_naive_bayes(row['sentence'], log_prior, log_likelihood, [0, 1, 2, 3, 4], vocab)\n",
    "    if predicted_class == row['label']:\n",
    "        cnt+=1\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = cnt/len(train_df)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy}')\n",
    "    \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7468398876404494\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:12:27.006865Z",
     "start_time": "2024-10-31T00:11:58.019924Z"
    }
   },
   "source": [
    "cnt=0\n",
    "for _, row in test_df.iterrows():\n",
    "    predicted_class=test_naive_bayes(row['sentence'], log_prior, log_likelihood, [0, 1, 2, 3, 4], vocab)\n",
    "    if predicted_class == row['label']:\n",
    "        cnt+=1\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = cnt/len(test_df)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Number of wrong predicted sentences:{len(test_df)-cnt}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.38552036199095024\n",
      "Number of wrong predicted sentences:1358\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T00:07:35.931498Z",
     "start_time": "2024-10-31T00:07:35.925362Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(vocab))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18280\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metrics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        matrix[t][p] += 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        tp = conf_matrix[c, c]\n",
    "        fp = conf_matrix[:, c].sum() - tp\n",
    "        fn = conf_matrix[c, :].sum() - tp\n",
    "\n",
    "        precision_c = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_c = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_c = 2 * precision_c * recall_c / (precision_c + recall_c) if (precision_c + recall_c) > 0 else 0\n",
    "\n",
    "        precision.append(precision_c)\n",
    "        recall.append(recall_c)\n",
    "        f1_score.append(f1_c)\n",
    "\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1_score)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Class\": [\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"],\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    })\n",
    "\n",
    "    metrics_df.loc[len(metrics_df)] = [\"Macro Average\", macro_precision, macro_recall, macro_f1]\n",
    "\n",
    "    return metrics_df\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_confusion_matrix_table(conf_matrix, class_labels):\n",
    "    df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "true_labels = [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "pred_labels = [0, 1, 1, 3, 4, 0, 0, 2, 3, 4, 2, 1, 2, 3, 3]\n",
    "# Define the class labels\n",
    "class_labels = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "num_classes = 5\n",
    "\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, num_classes)\n",
    "# Print the confusion matrix in table format\n",
    "print_confusion_matrix_table(conf_matrix, class_labels)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = calculate_metrics(conf_matrix)\n",
    "print(metrics.to_string(index=False))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison With Sklearn"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T23:54:19.824816Z",
     "start_time": "2024-10-30T23:54:19.577594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.set_params(\n",
    "    vectorizer__max_features=len(vocab),  \n",
    "    vectorizer__ngram_range=(1, 1), \n",
    "    vectorizer__binary=False , \n",
    ")\n",
    "\n",
    "pipeline.set_params(classifier__alpha=1)  # Laplace smoothing\n",
    "\n",
    "pipeline.fit(train_df['sentence'], train_df['label'])\n",
    "predictions = pipeline.predict(test_df['sentence'])\n",
    "\n",
    "# compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4090497737556561\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
