{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:42:43.291239Z",
     "start_time": "2024-10-31T14:42:43.219384Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16579\n",
      "Document-Term Matrix shape: (8544, 16579)\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and preprocess text\n",
    "def tokenize(text):\n",
    "    return text.lower().split()  # Simple split for tokenization; can enhance as needed\n",
    "\n",
    "# Function to generate vocabulary\n",
    "def generate_vocab(train_df):\n",
    "    tokens = set()\n",
    "    for _, row in train_df.iterrows():\n",
    "        row_tokens = tokenize(row['sentence'])\n",
    "        tokens.update(row_tokens)  # Use set to avoid duplicates\n",
    "    return np.array(sorted(tokens))  # Return sorted array for consistent ordering\n",
    "\n",
    "# Function to create the document-term matrix\n",
    "def create_document_term_matrix(train_df, vocab):\n",
    "    # Initialize a matrix with zeros\n",
    "    doc_term_matrix = np.zeros((len(train_df), len(vocab)), dtype=int)\n",
    "    \n",
    "    # Create a mapping of words to their indices\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    for doc_index, (_, row) in enumerate(train_df.iterrows()):\n",
    "        tokens = tokenize(row['sentence'])  # Tokenize the document\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:  # Check if the token is in the vocabulary\n",
    "                doc_term_matrix[doc_index][word_to_index[token]] += 1  # Increment count\n",
    "                \n",
    "    return doc_term_matrix\n",
    "\n",
    "# Function to get the number of words in each class\n",
    "def get_number_of_words_in_class(df):\n",
    "    class_count = {}\n",
    "    for _, row in df.iterrows():\n",
    "        row_tokens = tokenize(row['sentence'])\n",
    "        if row['label'] in class_count:\n",
    "            class_count[row['label']] += len(row_tokens)\n",
    "        else:\n",
    "            class_count[row['label']] = len(row_tokens)\n",
    "    return class_count\n",
    "\n",
    "# Example usage\n",
    "# Assuming train_df is your DataFrame with 'sentence' and 'label' columns\n",
    "vocab = generate_vocab(train_df)\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "\n",
    "# Create the document-term matrix\n",
    "doc_term_matrix = create_document_term_matrix(train_df, vocab)\n",
    "print(f'Document-Term Matrix shape: {doc_term_matrix.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T14:47:02.887190Z",
     "start_time": "2024-10-31T14:47:00.657254Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:42:45.825696Z",
     "start_time": "2024-10-31T14:42:44.771336Z"
    }
   },
   "source": [],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18278\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            sentence  label  \\\n0  The Rock is destined to be the 21st Century 's...      3   \n1  The gorgeously elaborate continuation of `` Th...      4   \n2  Singer\\/composer Bryan Adams contributes a sle...      3   \n3  You 'd think by now America would have had eno...      2   \n4               Yet the act is still charming here .      3   \n\n                                              tokens    score  \n0  The|Rock|is|destined|to|be|the|21st|Century|'s...  0.69444  \n1  The|gorgeously|elaborate|continuation|of|``|Th...  0.83333  \n2  Singer\\/composer|Bryan|Adams|contributes|a|sle...  0.62500  \n3  You|'d|think|by|now|America|would|have|had|eno...  0.50000  \n4               Yet|the|act|is|still|charming|here|.  0.72222  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>tokens</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Rock is destined to be the 21st Century 's...</td>\n      <td>3</td>\n      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n      <td>0.69444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The gorgeously elaborate continuation of `` Th...</td>\n      <td>4</td>\n      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n      <td>0.83333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n      <td>3</td>\n      <td>Singer\\/composer|Bryan|Adams|contributes|a|sle...</td>\n      <td>0.62500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You 'd think by now America would have had eno...</td>\n      <td>2</td>\n      <td>You|'d|think|by|now|America|would|have|had|eno...</td>\n      <td>0.50000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yet the act is still charming here .</td>\n      <td>3</td>\n      <td>Yet|the|act|is|still|charming|here|.</td>\n      <td>0.72222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:48:28.905741Z",
     "start_time": "2024-10-31T14:48:26.711740Z"
    }
   },
   "source": [
    "# Calculate class word counts\n",
    "class_count = get_number_of_words_in_class(train_df)\n",
    "\n",
    "# Calculate prior probabilities\n",
    "n_doc = len(train_df)\n",
    "log_prior = np.log(train_df['label'].value_counts().sort_index() / n_doc).tolist()\n",
    "\n",
    "# Precompute log likelihood\n",
    "log_likelihood = np.zeros((len(vocab), 5))  # Assuming 5 classes\n",
    "\n",
    "for c in range(5):  # For each class\n",
    "    # Get total word counts for the class using the document-term matrix\n",
    "    n_wc = np.sum(doc_term_matrix[train_df['label'] == c], axis=0)\n",
    "    for index, w in enumerate(vocab):\n",
    "        # Calculate log likelihood for each word in vocab\n",
    "        log_likelihood[index][c] = np.log((n_wc[index] + 1) / (class_count[c] + len(vocab)))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T14:48:44.242249Z",
     "start_time": "2024-10-31T14:48:44.233543Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
    "    # Create a dictionary to map words to their indices for quick lookup\n",
    "    word_to_index = {word: i for i, word in enumerate(V)}\n",
    "    sum_scores = np.zeros(len(C))  # Initialize scores for each class\n",
    "    \n",
    "    for c in range(len(C)):\n",
    "        # Initialize score with the log prior for the class\n",
    "        sum_scores[c] = logprior[c]\n",
    "        \n",
    "        for word in testdoc.split():  # Split the test document into words\n",
    "            if word in word_to_index:  # Check if the word is in V\n",
    "                # Update score by adding loglikelihood of the word given the class\n",
    "                index = np.where(vocab == word)[0]\n",
    "                sum_scores[c] += loglikelihood[index[0]][c]\n",
    "                \n",
    "    # Return the class with the maximum score\n",
    "    return np.argmax(sum_scores)\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:52:21.355716Z",
     "start_time": "2024-10-31T14:48:46.844678Z"
    }
   },
   "source": [
    "cnt=0\n",
    "for _, row in train_df.iterrows():\n",
    "    predicted_class=test_naive_bayes(row['sentence'], log_prior, log_likelihood, [0, 1, 2, 3, 4], vocab)\n",
    "    if predicted_class == row['label']:\n",
    "        cnt+=1\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = cnt/len(train_df)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy}')\n",
    "    \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7400514981273408\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:53:18.980635Z",
     "start_time": "2024-10-31T14:52:24.416737Z"
    }
   },
   "source": [
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "cnt=0\n",
    "for _, row in test_df.iterrows():\n",
    "    predicted_class=test_naive_bayes(row['sentence'], log_prior, log_likelihood, [0, 1, 2, 3, 4], vocab)\n",
    "    true_labels.append(row['label'])\n",
    "    predicted_labels.append(predicted_class)\n",
    "    if predicted_class == row['label']:\n",
    "        cnt+=1\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = cnt/len(test_df)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Number of wrong predicted sentences:{len(test_df)-cnt}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.40090497737556563\n",
      "Number of wrong predicted sentences:1324\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:31:14.174809Z",
     "start_time": "2024-10-31T01:31:14.169479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(vocab))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18278\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:51:12.564274Z",
     "start_time": "2024-10-31T01:51:12.549225Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 67,
   "source": [
    "\n",
    "def compute_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(true_labels, pred_labels):\n",
    "        matrix[t][p] += 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        tp = conf_matrix[c, c]\n",
    "        fp = conf_matrix[:, c].sum() - tp\n",
    "        fn = conf_matrix[c, :].sum() - tp\n",
    "\n",
    "        precision_c = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_c = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_c = 2 * precision_c * recall_c / (precision_c + recall_c) if (precision_c + recall_c) > 0 else 0\n",
    "\n",
    "        precision.append(precision_c)\n",
    "        recall.append(recall_c)\n",
    "        f1_score.append(f1_c)\n",
    "\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1_score)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Class\": [\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"],\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1_score\n",
    "    })\n",
    "\n",
    "    metrics_df.loc[len(metrics_df)] = [\"Macro Average\", macro_precision, macro_recall, macro_f1]\n",
    "\n",
    "    return metrics_df\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:34:28.487543Z",
     "start_time": "2024-10-31T01:34:28.482333Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 58,
   "source": [
    "def print_confusion_matrix_table(conf_matrix, class_labels):\n",
    "    df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(df)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:54:59.811632Z",
     "start_time": "2024-10-31T01:54:59.797957Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "         Class 0  Class 1  Class 2  Class 3  Class 4\n",
      "Class 0       11      205        5       58        0\n",
      "Class 1        9      395       38      181       10\n",
      "Class 2        2      167       21      193        6\n",
      "Class 3        1       98       11      366       34\n",
      "Class 4        1       39        7      293       59\n"
     ]
    }
   ],
   "execution_count": 92,
   "source": [
    "\n",
    "# Define the class labels\n",
    "class_labels = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "num_classes = 5\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(true_labels, predicted_labels, num_classes)\n",
    "# Print the confusion matrix in table format\n",
    "print_confusion_matrix_table(conf_matrix, class_labels)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:55:04.670461Z",
     "start_time": "2024-10-31T01:55:04.658281Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class  Precision   Recall  F1-Score\n",
      "      Class 0   0.458333 0.039427  0.072607\n",
      "      Class 1   0.436947 0.624013  0.513988\n",
      "      Class 2   0.256098 0.053985  0.089172\n",
      "      Class 3   0.335472 0.717647  0.457214\n",
      "      Class 4   0.541284 0.147870  0.232283\n",
      "Macro Average   0.405627 0.316588  0.273053\n"
     ]
    }
   ],
   "execution_count": 93,
   "source": [
    "metrics = calculate_metrics(conf_matrix)\n",
    "print(metrics.to_string(index=False))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison With Sklearn"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:58:51.052439Z",
     "start_time": "2024-10-31T14:58:50.571811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.set_params(\n",
    "    vectorizer__max_features=len(vocab),  \n",
    "    vectorizer__ngram_range=(1, 1), \n",
    "    vectorizer__binary=False , \n",
    ")\n",
    "\n",
    "pipeline.set_params(classifier__alpha=1)  # Laplace smoothing\n",
    "\n",
    "pipeline.fit(train_df['sentence'], train_df['label'])\n",
    "predictions = pipeline.predict(test_df['sentence'])\n",
    "\n",
    "# compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4090497737556561\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "         Class 0  Class 1  Class 2  Class 3  Class 4\n",
      "Class 0       10      215        7       47        0\n",
      "Class 1       15      429       35      147        7\n",
      "Class 2        4      167       17      193        8\n",
      "Class 3        1       92       17      371       29\n",
      "Class 4        1       41        9      295       53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix_sk = confusion_matrix(true_labels, predictions)\n",
    "print_confusion_matrix_table(conf_matrix_sk, class_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T02:11:53.419111Z",
     "start_time": "2024-10-31T02:11:53.408273Z"
    }
   },
   "execution_count": 231
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class  Precision   Recall  F1-Score\n",
      "      Class 0   0.322581 0.035842  0.064516\n",
      "      Class 1   0.454449 0.677725  0.544071\n",
      "      Class 2   0.200000 0.043702  0.071730\n",
      "      Class 3   0.352327 0.727451  0.474728\n",
      "      Class 4   0.546392 0.132832  0.213710\n",
      "Macro Average   0.375150 0.323510  0.273751\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(conf_matrix_sk)\n",
    "print(metrics.to_string(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T02:11:55.886705Z",
     "start_time": "2024-10-31T02:11:55.875484Z"
    }
   },
   "execution_count": 232
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Class  Precision   Recall  F1-Score\n",
      "      Class 0   0.458333 0.039427  0.072607\n",
      "      Class 1   0.436947 0.624013  0.513988\n",
      "      Class 2   0.256098 0.053985  0.089172\n",
      "      Class 3   0.335472 0.717647  0.457214\n",
      "      Class 4   0.541284 0.147870  0.232283\n",
      "Macro Average   0.405627 0.316588  0.273053\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(conf_matrix)\n",
    "print(metrics.to_string(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T02:03:34.430811Z",
     "start_time": "2024-10-31T02:03:34.418732Z"
    }
   },
   "execution_count": 165
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
